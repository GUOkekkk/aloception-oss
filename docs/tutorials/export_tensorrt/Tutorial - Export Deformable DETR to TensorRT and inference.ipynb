{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import alonet\n",
    "from alonet.common import pl_helpers\n",
    "# for Deformable DETR\n",
    "from alonet.deformable_detr import (\n",
    "    DeformableDETR, \n",
    "    DeformableDetrR50Refinement, \n",
    "    DeformableDetrR50RefinementFinetune,\n",
    "    LitDeformableDetr\n",
    ")\n",
    "# for TensorRT\n",
    "from alonet.deformable_detr.trt_exporter import (\n",
    "    DeformableDetrTRTExporter, \n",
    "    load_trt_plugins_for_deformable_detr)\n",
    "from alonet.torch2trt import TRTExecutor\n",
    "\n",
    "import aloscene\n",
    "from aloscene import Frame\n",
    "\n",
    "# Deformable DETR requires custom TensorRT plugin for Multiscale Deformable Attention\n",
    "load_trt_plugins_for_deformable_detr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will convert Deformable DETR in TensorRT in order to reduce memory footprint and inference time.\n",
    "\n",
    "The model weight can be loaded either from a .pth file or from a run_id using `Aloception` API. \n",
    "\n",
    "This notebook might crash if there is not enough GPU memory. In this case you can reduce the image size or run only cells from either **Load from .pth checkpoint** or **Inference with TensorRT** or **Load weight from run_id**.\n",
    "\n",
    "The workflow is:\\\n",
    "1.Load model and trained weights\\\n",
    "2.Instantiate corresponding TensorRT exporter \\\n",
    "3.Run the export\\\n",
    "4.Grab a cup of coffee or take some air while waiting :)\n",
    "\n",
    "Now, let's define some constant what we will use throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = [3, 1280, 1920] # [C, H, W], change input shape if needed\n",
    "BATCH_SIZE = 1\n",
    "PRECISION = \"fp16\" # or \"fp32\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dimension is [B, C, H, W] of which [C, H, W] is defined by `INPUT_SHAPE` and B is defined by `BATCH_SIZE`.\n",
    "\n",
    "`PRECISION` defines the precision of model weights. It is either \"fp32\" or \"fp16\" for float32 and float16 respectively.\n",
    "\n",
    "We will run inference in a test image for qualitative comparison between PyTorch and TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"PATH/TO/IMAGE\"\n",
    "img = Frame(image_path)\n",
    "frame = img.resize(INPUT_SHAPE[1:]).norm_resnet()\n",
    "frame = Frame.batch_list([frame])\n",
    "img.get_view().render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deformable DETR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from .pth checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use weight Deformable DETR R50 with iterative box refinement trained on COCO from [official repository Deformable DETR](https://github.com/fundamentalvision/Deformable-DETR) but the workflow is valid for any finetuned model with its associated .pth file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate model and load trained weight\n",
    "weight_path = \"PATH/TO/CHECKPOINT.pth\"\n",
    "num_classes = background_class = 91 # COCO classes\n",
    "\n",
    "torch_model = DeformableDetrR50Refinement(\n",
    "    num_classes=num_classes,\n",
    "    aux_loss=False, # we don't want auxilary outputs\n",
    ")\n",
    "torch_model.eval()\n",
    "alonet.common.load_weights(torch_model, weight_path, torch_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instantiate corresponding exporter\n",
    "\n",
    "model_name = \"\".join(os.path.basename(weight_path).split(\".\")[:-1])\n",
    "# Because the exporter will use ONNX format as an intermediate bridge \n",
    "# between PyTorch and TensorRT, we need to specify a path where the ONNX file will be save.\n",
    "onnx_path = os.path.join(os.path.dirname(weight_path), model_name + \".onnx\")\n",
    "\n",
    "exporter = DeformableDetrTRTExporter(\n",
    "    model=torch_model,\n",
    "    onnx_path=onnx_path,\n",
    "    input_shapes=(INPUT_SHAPE,),\n",
    "    input_names=[\"img\"], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    precision=PRECISION,\n",
    "    device=torch_model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3. Run the exporter\n",
    "exporter.export_engine()\n",
    "engine_path = exporter.engine_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the export, 2 files will be created in the root directory containing the checkpoint file.\n",
    "\n",
    "ROOT_DIR\\\n",
    "|__ MODEL.pth\\\n",
    "|__ MODEL.onnx\\\n",
    "|__ MODEL_PRECISION.engine\n",
    "\n",
    "The .onnx file is a ONNX graph which serves as intermediate bridge between PyTorch and TensorRT. The .engine file is the model serialized as TensorRT engine. For deployment and inference, .engine file will be deserialized and executed by TensorRT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableDetrInference():\n",
    "    def __init__(self, activation_fn=\"sigmoid\", background_class=None):\n",
    "        load_trt_plugins_for_deformable_detr()\n",
    "        self.background_class = background_class\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def get_outs_filter(self, *args, **kwargs):\n",
    "        return DeformableDETR.get_outs_filter(self, *args, **kwargs)\n",
    "\n",
    "    def __call__(self, forward_out, **kwargs):\n",
    "        forward_out = {key: torch.tensor(forward_out[key]) for key in forward_out}\n",
    "        forward_out[\"activation_fn\"] = self.activation_fn\n",
    "        return DeformableDETR.inference(self, forward_out, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other to benefit the inference logic implemented in alonet Deformable DETR without instantiating the whole model in PyTorch, we create a helper class which calls `DeformableDETR.inference` method.\n",
    "\n",
    "In alonet we implemented the classification head with either sigmoid activation or softmax activation. In order to use the `DeformableDETR.inference` correctly, we need to define `activation_fn` and `background_class` in case of softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trt_model = TRTExecutor(engine_path)\n",
    "trt_model.print_bindings_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input `img` shape is (B, C, H, W) with C=4 because we concatenate RGB image (B, 3, H, W) and its mask of shape (B, 1, H, W) containing 1 on padded pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_input = np.concatenate([frame.as_tensor(), frame.mask.as_tensor()], axis=1, dtype=np.float32)\n",
    "\n",
    "trt_m_outputs = trt_model(m_input)\n",
    "trt_pred_boxes = DeformableDetrInference()(trt_m_outputs)\n",
    "\n",
    "# visualize the result\n",
    "trt_pred_boxes[0].get_view(frame=img).render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compare with the result from model in PyTorch\n",
    "with torch.no_grad():\n",
    "    torch_m_outputs = torch_model(frame.to(torch_model.device))\n",
    "torch_pred_boxes = torch_model.inference(torch_m_outputs)\n",
    "torch_pred_boxes[0].get_view(frame=img).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick qualitative comparison show that 2 models give nearly identical results. The minor difference is from the fact that we use the precision float16 for our TensorRT engine which is not the case for the model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weight from run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having trained your DETR model using aloception API, we can load the model from a run_id and export it to TensorRT using the same workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train project and the run_id from which we want to load weight\n",
    "project = \"YOUR_PROJECT_NAME\" \n",
    "run_id = \"YOUR_RUN_ID\"\n",
    "model_name = \"MODEL_NAME\" \n",
    "num_classes = ... # number of classes in your finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the model and load weight from run_id\n",
    "torch_model = DeformableDetrR50RefinementFinetune(\n",
    "    num_classes=num_classes, # only person class\n",
    "    aux_loss=False, # we don't want auxilary outputs\n",
    ")\n",
    "\n",
    "lit_model = pl_helpers.load_training(\n",
    "    LitDeformableDetr, # The PyTorch Lightning Module that was used in training\n",
    "    project_run_id=project, \n",
    "    run_id=run_id, \n",
    "    model=torch_model\n",
    ")\n",
    "torch_model = lit_model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instantiate the exporter\n",
    "# Because the exporter will use ONNX format as an intermediate bridge \n",
    "# between PyTorch and TensorRT, we need to specify a path where the ONNX file will be save.\n",
    "project_dir, run_id_dir, _ = pl_helpers.get_expe_infos(project, run_id)\n",
    "onnx_path = os.path.join(run_id_dir, model_name + \".onnx\")\n",
    "\n",
    "exporter = DeformableDetrTRTExporter(\n",
    "    model=torch_model,\n",
    "    onnx_path=onnx_path,\n",
    "    input_shapes=(INPUT_SHAPE,),\n",
    "    input_names=[\"img\"], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    precision=PRECISION,\n",
    "    device=torch_model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3. Run the exporter\n",
    "exporter.export_engine()\n",
    "engine_path = exporter.engine_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "trt_model = TRTExecutor(engine_path)\n",
    "trt_model.print_bindings_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableDetrInference():\n",
    "    def __init__(self, activation_fn=\"sigmoid\", background_class=None):\n",
    "        load_trt_plugins_for_deformable_detr()\n",
    "        self.background_class = background_class\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def get_outs_filter(self, *args, **kwargs):\n",
    "        return DeformableDETR.get_outs_filter(self, *args, **kwargs)\n",
    "\n",
    "    def __call__(self, forward_out, **kwargs):\n",
    "        forward_out = {key: torch.tensor(forward_out[key]) for key in forward_out}\n",
    "        forward_out[\"activation_fn\"] = self.activation_fn\n",
    "        return DeformableDETR.inference(self, forward_out, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "m_input = np.concatenate([frame.as_tensor(), frame.mask.as_tensor()], axis=1, dtype=np.float32)\n",
    "\n",
    "trt_m_outputs = trt_model(m_input)\n",
    "# In DeformableDetrInference, use `activation_fn` and `background_class` if needed\n",
    "trt_pred_boxes = DeformableDetrInference()(trt_m_outputs)\n",
    "\n",
    "# visualize the result\n",
    "trt_pred_boxes[0].get_view(frame=img).render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with the result from model in PyTorch\n",
    "with torch.no_grad():\n",
    "    torch_m_outputs = torch_model(frame.to(torch_model.device))\n",
    "torch_pred_boxes = torch_model.inference(torch_m_outputs)\n",
    "torch_pred_boxes[0].get_view(frame=img).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, the comparison show that 2 models give nearly identical results. The difference is from the fact that we use the precision float16 for our TensorRT engine. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68220423dd456e0c43a9c8931fec78ccc1b1e66482ae3da0e7135ef14a46e76e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('aloception': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
